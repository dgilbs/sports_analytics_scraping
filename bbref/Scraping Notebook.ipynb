{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9a63d1-a1f7-426c-a56c-b84b8a8c0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import ssl\n",
    "import warnings\n",
    "from sqlalchemy import create_engine, text\n",
    "from psycopg2.extras import execute_values\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6f78cf-824e-4992-b0dc-0e9446d27675",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ce62c-093c-400c-b04b-c84bc4b54dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5076cf8b-9031-4aaa-aabb-0d759b0cdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"info.yaml\", 'r') as stream:\n",
    "    info = yaml.safe_load(stream)\n",
    "\n",
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "with open(\"db_config.yaml\", 'r') as stream:\n",
    "    db_config = yaml.safe_load(stream)\n",
    "\n",
    "wnba = info['WNBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf242ca-393c-4fc3-a1c2-9c216b9c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_schedule(config, info, season):\n",
    "    league_name = info['name']\n",
    "    league_tag = info['url_tag']\n",
    "    url = \"https://www.basketball-reference.com/{}/years/{}_games.html\".format(league_tag, str(season))\n",
    "    df = pd.read_html(url, extract_links='body')[0]\n",
    "    dir_path = 'raw_data/schedules/{}'.format(league_name)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    file_name = '{}_schedule.pkl'.format(season)\n",
    "    rfp = os.path.join(dir_path, file_name)\n",
    "    df.to_pickle(rfp)\n",
    "    if 'Notes' in df.columns:\n",
    "        df.columns = config['schedule_rename_columns']\n",
    "    else:\n",
    "        df.columns = config['schedule_rename_columns'][:-1]\n",
    "    link_cols = config['schedule_link_columns']\n",
    "    non_link_cols = [i for i in df.columns if i not in link_cols]\n",
    "    for i in link_cols:\n",
    "        new_col = i + '_link'\n",
    "        df[new_col] = df.apply(lambda row: row[i][1], axis=1)\n",
    "        df[i] = df.apply(lambda row: row[i][0], axis=1)\n",
    "\n",
    "    for j in non_link_cols:\n",
    "        df[j] = df.apply(lambda row: row[j][0], axis=1)\n",
    "    first_playoff_index = df[df['home_team'] == 'Playoffs'].index\n",
    "    \n",
    "    df = df[~pd.isnull(df.away_team_link)]\n",
    "    if not first_playoff_index.empty:\n",
    "        first_playoff_index = first_playoff_index[0]\n",
    "        # Create the 'is_playoffs' column based on the index\n",
    "        df['is_playoffs'] = df.index > first_playoff_index\n",
    "    else:\n",
    "        # If 'Playoffs' is not found, set all to False\n",
    "        df['is_playoffs'] = False\n",
    "    if 'notes' in df.columns:\n",
    "        df['is_commissioners_cup'] = df.notes.str.contains(\"Commissioner's Cup Game\")\n",
    "    else:\n",
    "        df['is_commissioners_cup'] = False\n",
    "    df['away_team_id'] = df.apply(lambda row: row['away_team_link'].split('/')[-2], axis=1)\n",
    "    df['home_team_id'] = df.apply(lambda row: row['home_team_link'].split('/')[-2], axis=1)\n",
    "    df['game_id'] = df.apply(lambda row: row['box_score_link'].split('.')[0].split('/')[-1], axis=1)\n",
    "    df['game_date'] = df.apply(lambda row: datetime.strptime(row['game_date'], \"%a, %b %d, %Y\").strftime(\"%Y-%m-%d\"), axis=1)\n",
    "\n",
    "    dir_path = 'data/schedules/{}'.format(league_name)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    file_name = '{}_schedule.pkl'.format(season)\n",
    "    fp = os.path.join(dir_path, file_name)\n",
    "    df.to_pickle(fp)\n",
    "    return df\n",
    "\n",
    "\n",
    "def scrape_multiple_season_schedules(config, info, seasons):\n",
    "    for season in seasons:\n",
    "        scrape_schedule(config, wnba, season)\n",
    "        time.sleep(10)\n",
    "\n",
    "def extract_table_ids(url):\n",
    "    if 'https://basketball-reference.com' not in url:\n",
    "        url = 'https://basketball-reference.com' + url\n",
    "    response = requests.get(url, verify=False)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parse with BeautifulSoup to find tables with IDs\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tables_with_ids = []\n",
    "    \n",
    "    # Loop through each table, extract the ID and data\n",
    "    for table in soup.find_all('table'):\n",
    "        table_id = table.get('id')\n",
    "        tables_with_ids.append(table_id)\n",
    "\n",
    "    return tables_with_ids\n",
    "\n",
    "def scrape_box_score(row, info):\n",
    "    url = 'https://basketball-reference.com' + row['box_score_link']\n",
    "    league_name = info['name']\n",
    "    basic_dir_path = 'raw_data/box_scores/{}/basic'.format(league_name)\n",
    "    advanced_dir_path = 'raw_data/box_scores/{}/advanced'.format(league_name)\n",
    "    if not os.path.exists(basic_dir_path):\n",
    "        os.makedirs(basic_dir_path)\n",
    "\n",
    "    if not os.path.exists(advanced_dir_path):\n",
    "        os.makedirs(advanced_dir_path)\n",
    "    table_ids = extract_table_ids(url)\n",
    "    arr = pd.read_html(url, extract_links='body')\n",
    "    final_dict = {}\n",
    "    for index, tid in enumerate(table_ids):\n",
    "        if tid is not None and ('-q' in tid or 'ot' in tid):\n",
    "            temp_df = arr[index]\n",
    "            final_dict[tid] = temp_df\n",
    "            team = tid.split('-')[1]\n",
    "            temp_df['team_id'] = team\n",
    "            temp_df['game_id'] = row['game_id']\n",
    "            quarter = tid.split('-')[2].upper()\n",
    "            temp_df['game_quarter'] = quarter\n",
    "            dir_path = basic_dir_path\n",
    "            file_name = row['game_id'] + '_' + tid.replace('-', '_') + '.pkl'\n",
    "        elif tid is not None and 'advanced' in tid:\n",
    "            temp_df = arr[index]\n",
    "            final_dict[tid] = temp_df\n",
    "            team = tid.split('-')[0]\n",
    "            temp_df['team_id'] = team\n",
    "            temp_df['game_id'] = row['game_id']\n",
    "            dir_path = advanced_dir_path\n",
    "            file_name = row['game_id'] + '_' + tid.replace('-', '_') + '.pkl'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        fp = os.path.join(dir_path, file_name)\n",
    "        temp_df.to_pickle(fp)\n",
    "\n",
    "\n",
    "def scrape_roster(row, info, home=True):\n",
    "    if home:\n",
    "        url = row['home_team_link']\n",
    "        team_id = row['home_team_id']\n",
    "    else:\n",
    "        url = row['away_team_link']\n",
    "        team_id = row['away_team_id']\n",
    "    league = info['name']\n",
    "    season = url.split('/')[-1].split('.')[0]\n",
    "    if 'https://basketball-reference.com' not in url:\n",
    "        url = 'https://basketball-reference.com' + url\n",
    "    attrs={'id': 'roster'}\n",
    "    df = pd.read_html(url, attrs=attrs, extract_links='body')[0]\n",
    "    df['season'] = season\n",
    "    df['team_id'] = team_id\n",
    "    dir_path = 'raw_data/rosters/{}'.format(league)\n",
    "    file_name = '{}_{}_roster.pkl'.format(team_id, season)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    fp = os.path.join(dir_path, file_name)\n",
    "    df.to_pickle(fp)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_roster(fp, info, config):\n",
    "    df = pd.read_pickle(fp)\n",
    "    dir, file = os.path.split(fp)\n",
    "    df.columns = [i.lower().replace(' ', '_') for i in df.columns]\n",
    "    rename_cols = config['roster_rename_columns']\n",
    "    df = df.rename(columns=rename_cols)\n",
    "    link_cols = config['roster_link_columns']\n",
    "    non_link_cols = [i for i in df.columns if i not in link_cols]\n",
    "    \n",
    "    for i in link_cols:\n",
    "        new_col = i + '_link'\n",
    "        df[new_col] = df.apply(lambda row: row[i][1], axis=1)\n",
    "        df[i] = df.apply(lambda row: row[i][0], axis=1)\n",
    "    for j in non_link_cols:\n",
    "        df[j] = df.apply(lambda row: row[j][0], axis=1)\n",
    "\n",
    "    #df['college_grad'] = df.apply(lambda row: [i.strip() for i in row['colleges'] if i != ''], axis=1)\n",
    "    df['last_college'] = df.apply(lambda row: row['colleges'].split(', ')[-1] if row['colleges'] != '' else None, axis=1)\n",
    "    team_id = fp.split('/')[-1].split('_')[0]\n",
    "    df['team_id'] = team_id\n",
    "    df['player_id'] = df.apply(lambda row: row['player_link'].split('/')[-1].split('.')[0], axis=1)\n",
    "    df['season'] = file.split('_')[1]\n",
    "    df['id'] = df['player_id'] + '-' + df['team_id'] + '-' + df['season']\n",
    "    df['height_inches'] = df.apply(lambda row: height_str_to_inches(row['height']), axis=1)\n",
    "    dir_path = 'data/rosters/{}/'.format(info['name'])\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    nfp = os.path.join(dir_path, file)\n",
    "    df.to_pickle(nfp)\n",
    "    return df\n",
    "\n",
    "def scrape_box_scores_from_schedule(schedule, info):\n",
    "    for index, i in schedule.iterrows():\n",
    "        print(index, i['game_date'], i['game_id'])\n",
    "        try:\n",
    "            scrape_box_score(i, info)\n",
    "            time.sleep(10)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "\n",
    "def cast_dtypes(df, datatypes):\n",
    "    \"\"\"\n",
    "    Casts datatypes to columns in a database\n",
    "\n",
    "    Args:\n",
    "        df(DataFrame): DataFrame to assign dtypes to\n",
    "        dtypes(dict): dict of columns and their corresponding datatypes\n",
    "\n",
    "    Returns:\n",
    "        new_df(DataFrame): dataframe with reset datatypes\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    arr = list(datatypes.keys())\n",
    "    for i in arr:\n",
    "        if i in df.columns:\n",
    "            new_df[i] = new_df[i].fillna(np.nan)\n",
    "            new_df[i] = new_df[i].replace('', np.nan)\n",
    "#             if df[i].dtype == 'object':\n",
    "#                 print(i)\n",
    "#                 new_df[i] = new_df[i].str.replace(',', '')\n",
    "            new_df[i] = new_df[i].astype(datatypes[i])\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def extract_player_id(player_link_str):\n",
    "    try:\n",
    "        value = player_link_str.split('/')[-1].split('.')[0]\n",
    "    except:\n",
    "        value = None\n",
    "    return value\n",
    "\n",
    "def clean_box_score(file_path, config, info, del_raw=False):\n",
    "    if 'basic' in file_path:\n",
    "        basic = True\n",
    "    else:\n",
    "        basic = False\n",
    "    league_name = info['name']\n",
    "    df = pd.read_pickle(file_path)\n",
    "    cols = list()\n",
    "    for col in df.columns:\n",
    "        if basic and col[0] == 'Basic Box Score Stats':\n",
    "            new_col = col[1].lower()\n",
    "        elif basic:\n",
    "            new_col = col[0].lower()\n",
    "        elif not basic and col[0] == 'Advanced Box Score Stats': \n",
    "            new_col = col[1].lower()\n",
    "        else: \n",
    "            new_col = col[0].lower()\n",
    "        cols.append(new_col)\n",
    "    df.columns = cols\n",
    "    if basic:\n",
    "        rename_columns = config['basic_box_score_rename_columns']\n",
    "        link_cols = config['basic_box_score_link_columns']\n",
    "        non_link_cols = config['basic_box_score_non_link_columns']\n",
    "    else:\n",
    "        rename_columns = config['advanced_box_score_rename_columns']\n",
    "        link_cols = config['advanced_box_score_link_columns']\n",
    "        non_link_cols = config['advanced_box_score_non_link_columns']\n",
    "        \n",
    "\n",
    "    df = df.rename(columns=rename_columns)\n",
    "    \n",
    "    df = df[~df.player.isin(['Totals'])]\n",
    "    for i in link_cols:\n",
    "        new_col = i + '_link'\n",
    "        df[new_col] = df.apply(lambda row: row[i][1], axis=1)\n",
    "        df[i] = df.apply(lambda row: row[i][0], axis=1)\n",
    "    for j in non_link_cols:\n",
    "        df[j] = df.apply(lambda row: row[j][0], axis=1)\n",
    "\n",
    "    dir_path = 'data/box_scores/{}'.format(league_name)\n",
    "    file_name = os.path.split(file_path)[-1]\n",
    "    new_file_path = os.path.join(dir_path, file_name)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    df = df[(df.player != 'Reserves') & (df.minutes_played != '')]\n",
    "    df = cast_dtypes(df, config['box_score_dtypes'])\n",
    "    df['minutes_played_str'] = '00:' + df.minutes_played\n",
    "    df['minutes_played_time'] = pd.to_timedelta(df.minutes_played_str)\n",
    "    df['minutes_played_int'] = df.apply(lambda row: round(row['minutes_played_time'].total_seconds()/60, 4), axis=1)\n",
    "    df['league'] = info['name']\n",
    "    df['player_id'] = df.apply(lambda row: extract_player_id(row['player_link']), axis=1)\n",
    "    if not basic: \n",
    "        df['id'] = df.apply(lambda row: row['player_id'] + '-' + row['game_id'] + '-' + 'adv', axis=1)\n",
    "    else:\n",
    "        df['id'] = df.apply(lambda row: row['player_id'] + '-' + row['game_id'] + '-' + row['game_quarter'], axis=1)\n",
    "    df.to_pickle(new_file_path)\n",
    "    return df\n",
    "\n",
    "def upsert_df_sqlite(df, table_name, db_config):\n",
    "    info = db_config[table_name]\n",
    "    cols = info['df_cols']\n",
    "    idf = df[cols]\n",
    "    idf.columns = info['rename_cols']\n",
    "    for col in idf.columns:\n",
    "        idf.loc[:, col] = idf[col].fillna(0)\n",
    "    idf = idf.drop_duplicates(subset=info['key'])\n",
    "    conn = sqlite3.connect('basketball.db')\n",
    "    cursor = conn.cursor()\n",
    "    columns = ', '.join(idf.columns)\n",
    "    placeholders = ', '.join(['?'] * len(idf.columns))\n",
    "    update_columns = ', '.join([f'{col}=excluded.{col}' for col in idf.columns if col != 'id'])\n",
    "    for index, row in idf.iterrows():\n",
    "        sql = f'''\n",
    "        INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\n",
    "        ON CONFLICT(id) DO UPDATE SET {update_columns}\n",
    "        '''\n",
    "        cursor.execute(sql, tuple(row))\n",
    "        \n",
    "        \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def height_str_to_inches(height_str):\n",
    "    \"\"\"Converts a height string in the format 'ft'in\"' to inches.\"\"\"\n",
    "    if height_str == '':\n",
    "        return None\n",
    "    feet, inches = height_str.split(\"-\")\n",
    "    feet = int(feet)\n",
    "    inches = int(inches.replace('\"', ''))\n",
    "\n",
    "    total_inches = feet * 12 + inches\n",
    "    return total_inches\n",
    "\n",
    "\n",
    "def build_team_schedules(schedule, info, config):\n",
    "    rows = list()\n",
    "    for index, row in schedule.iterrows():\n",
    "        away_row_id = row['game_id'] + '-' + row['away_team_id']\n",
    "        home_row_id = row['game_id'] + '-' + row['home_team_id']\n",
    "        game_date = row['game_date']\n",
    "        away_team_id = row['away_team_id']\n",
    "        home_team_id = row['home_team_id']\n",
    "        away_team_pts = row['away_pts']\n",
    "        home_team_pts = row['home_pts']\n",
    "        is_playoffs = row['is_playoffs']\n",
    "        is_commissioners_cup = row['is_commissioners_cup']\n",
    "        season = row['season']\n",
    "        game_id = row['game_id']\n",
    "        away_row = [away_row_id, game_date, away_team_id, home_team_id, away_team_pts, home_team_pts, is_playoffs, is_commissioners_cup, season, 'Away', game_id]\n",
    "        home_row = [home_row_id, game_date, home_team_id, away_team_id, home_team_pts, away_team_pts, is_playoffs, is_commissioners_cup,season, 'Home', game_id]\n",
    "        rows.append(home_row)\n",
    "        rows.append(away_row)\n",
    "    final = pd.DataFrame(rows, columns=config['team_schedule_columns'])\n",
    "    final['margin'] = final.team_pts.astype(int) - final.opponent_pts.astype(int)\n",
    "    final['game_win'] = final.apply(lambda row: int(row['team_pts']) > int(row['opponent_pts']), axis=1)\n",
    "    final['league'] = info['name']\n",
    "    \n",
    "    return final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf6acab-cb12-421a-87b5-0bf7226e1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_df(df, table_name, conn_string, unique_columns, db_config, schema='basketball', dedupe=False):\n",
    "    \"\"\"\n",
    "    Upserts a pandas DataFrame to a PostgreSQL table.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to upsert.\n",
    "        table_name (str): The name of the target table.\n",
    "        conn_string (str): PostgreSQL connection string.\n",
    "        unique_columns (list): List of column names that uniquely identify a row.\n",
    "    \"\"\"\n",
    "    config = db_config[table_name]\n",
    "    df_cols = config['df_cols']\n",
    "    rename_cols = config['rename_cols']\n",
    "    df = df[df_cols]\n",
    "    df.columns = rename_cols\n",
    "    if 'numeric_cols' in config:\n",
    "        num_cols = config['numeric_cols']\n",
    "        for col in num_cols:\n",
    "            df[col] = pd.to_numeric(df.loc[:, col], errors='coerce')\n",
    "            df[col] = df.loc[:, col].fillna(0)\n",
    "            #df[col] = df[col].replace({pd.NA: 0, np.nan: 0})\n",
    "\n",
    "    if dedupe:\n",
    "        df = df.drop_duplicates(subset=unique_columns, ignore_index=True)\n",
    "    table_id = '.'.join([schema, table_name])\n",
    "    df = df.map(lambda x: x.item() if isinstance(x, (pd.Int64Dtype, pd.Float64Dtype, pd.BooleanDtype)) else x)\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(conn_string)\n",
    "    with engine.connect() as conn:\n",
    "        with conn.begin():\n",
    "            # Ensure column names are valid SQL identifiers\n",
    "            df.columns = [col.lower() for col in df.columns]\n",
    "            \n",
    "            # Get column names\n",
    "            columns = list(df.columns)\n",
    "            \n",
    "            # Generate SQL placeholders\n",
    "            placeholders = ', '.join(['%s'] * len(columns))\n",
    "            columns_str = ', '.join(columns)\n",
    "            updates = ', '.join([f\"{col} = EXCLUDED.{col}\" for col in columns if col not in unique_columns])\n",
    "            \n",
    "            # Convert DataFrame to list of tuples\n",
    "            data = [tuple(row) for row in df.itertuples(index=False, name=None)]\n",
    "            \n",
    "            # Generate UPSERT query\n",
    "            upsert_query = f'''\n",
    "                INSERT INTO {table_id} ({columns_str})\n",
    "                VALUES {placeholders}\n",
    "                ON CONFLICT ({', '.join(unique_columns)})\n",
    "                DO UPDATE SET {updates};\n",
    "            '''\n",
    "            \n",
    "            # Use psycopg2 to execute query efficiently\n",
    "            with conn.connection.cursor() as cursor:\n",
    "                execute_values(cursor, upsert_query.replace(placeholders, '%s'), data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f44b00-882f-4373-a24f-308e2f1561fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"info.yaml\", 'r') as stream:\n",
    "    info = yaml.safe_load(stream)\n",
    "\n",
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "with open(\"db_config.yaml\", 'r') as stream:\n",
    "    db_config = yaml.safe_load(stream)\n",
    "\n",
    "wnba = info['WNBA']\n",
    "\n",
    "conn_string =  \"postgresql://danielgilberg:password@localhost:5432/projects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be03486-70e1-46cb-bc52-b4ba72900c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/box_scores/WNBA/'\n",
    "files = list()\n",
    "for i in os.listdir(folder):\n",
    "    if i[:4] == '2025' and 'basic' in i:\n",
    "        fp = os.path.join(folder, i)\n",
    "        df = pd.read_pickle(fp)\n",
    "        files.append(df)\n",
    "\n",
    "final = pd.concat(files, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96752416-e881-432a-8c1f-d34e7ca65bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = scrape_schedule(config, wnba, 2025)\n",
    "\n",
    "sched = sched[sched.box_score == 'Box Score']\n",
    "sched['season'] = '2025'\n",
    "sched['league'] = 'WNBA'\n",
    "\n",
    "folder = 'db_backup/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "results = build_team_schedules(sched, wnba, config)\n",
    "\n",
    "sched_config = db_config['dim_games']\n",
    "\n",
    "sched = sched[sched_config['df_cols']]\n",
    "sched.columns = sched_config['rename_cols']\n",
    "sfp = os.path.join(folder, 'dim_games.csv')\n",
    "# sched.to_csv(sfp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4805970a-0669-44a8-933b-2e96c69e6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/box_scores/WNBA/'\n",
    "arr = [i for i in os.listdir(folder) if '2025' in i and 'basic' in i]\n",
    "dfs = list()\n",
    "for i in arr:\n",
    "    fp = os.path.join(folder, i)\n",
    "    df = pd.read_pickle(fp)\n",
    "    dfs.append(df)\n",
    "\n",
    "final = pd.concat(dfs, ignore_index=True)\n",
    "idf = final[['player_id', 'player']].drop_duplicates(subset=['player_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fd6a2bf-e41a-4fd1-b5ee-bd08cabb819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df(idf, 'dim_players', conn_string, ['id'], db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f440916-e319-4ff4-a79d-15e1be7a379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched_config = db_config['dim_team_results']\n",
    "\n",
    "df = df[sched_config['df_cols']]\n",
    "df.columns = sched_config['rename_cols']\n",
    "sfp = os.path.join(folder, 'dim_team_results.csv')\n",
    "df.to_csv(sfp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b31c7dca-cfaa-426b-a03f-72ec2c791edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/schedules/WNBA/2025_schedule.pkl')\n",
    "table = 'dim_teams'\n",
    "temp_config = db_config[table]\n",
    "df = df[temp_config['df_cols']]\n",
    "df.columns = temp_config['rename_cols']\n",
    "fp = os.path.join(folder, '{}.csv'.format(table))\n",
    "df = df.drop_duplicates(subset='id')\n",
    "df.to_csv(fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "417fce10-249f-4ff8-b481-6c54e98484a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/box_scores/WNBA/'\n",
    "files = [i for i in os.listdir(folder) if 'basic' in i and '2025' in i]\n",
    "dfs = list()\n",
    "for file in files:\n",
    "    fp = os.path.join(folder, file)\n",
    "    df = pd.read_pickle(fp)\n",
    "    dfs.append(df)\n",
    "\n",
    "final = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08fe21c9-fc02-4b10-bfe2-56b9fc32cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final.copy()\n",
    "folder = 'db_backup/'\n",
    "table = 'dim_players'\n",
    "temp_config = db_config[table]\n",
    "df = df[temp_config['df_cols']]\n",
    "df.columns = temp_config['rename_cols']\n",
    "fp = os.path.join(folder, '{}.csv'.format(table))\n",
    "df = df.drop_duplicates(subset='id')\n",
    "df.to_csv(fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c05077f-8b62-4de7-b4a9-85902e73b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final.copy()\n",
    "folder = 'db_backup/'\n",
    "table = 'f_basic_box_score'\n",
    "temp_config = db_config[table]\n",
    "df = df[temp_config['df_cols']]\n",
    "df.columns = temp_config['rename_cols']\n",
    "fp = os.path.join(folder, '{}.csv'.format(table))\n",
    "df.to_csv(fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e8b3ce4-1b5b-4e4f-b433-f80780bb597d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LVA_2023_roster.pkl',\n",
       " 'DAL_2025_roster.pkl',\n",
       " 'IND_2019_roster.pkl',\n",
       " 'LAS_2020_roster.pkl',\n",
       " 'LAS_2017_roster.pkl',\n",
       " 'NYL_2018_roster.pkl',\n",
       " 'CHI_2017_roster.pkl',\n",
       " 'PHO_2024_roster.pkl',\n",
       " 'CHI_2020_roster.pkl',\n",
       " 'WAS_2017_roster.pkl',\n",
       " 'WAS_2020_roster.pkl',\n",
       " 'LVA_2019_roster.pkl',\n",
       " 'SEA_2025_roster.pkl',\n",
       " 'NYL_2022_roster.pkl',\n",
       " 'IND_2023_roster.pkl',\n",
       " 'WAS_2025_roster.pkl',\n",
       " 'CHI_2025_roster.pkl',\n",
       " 'PHO_2016_roster.pkl',\n",
       " 'PHO_2021_roster.pkl',\n",
       " 'SAS_2017_roster.pkl',\n",
       " 'MIN_2023_roster.pkl',\n",
       " 'CON_2023_roster.pkl',\n",
       " 'SEA_2020_roster.pkl',\n",
       " 'ATL_2022_roster.pkl',\n",
       " 'SEA_2017_roster.pkl',\n",
       " 'ATL_2018_roster.pkl',\n",
       " 'DAL_2020_roster.pkl',\n",
       " 'DAL_2017_roster.pkl',\n",
       " 'CON_2019_roster.pkl',\n",
       " 'MIN_2019_roster.pkl',\n",
       " 'LAS_2025_roster.pkl',\n",
       " 'LAS_2022_roster.pkl',\n",
       " 'LVA_2021_roster.pkl',\n",
       " 'WAS_2018_roster.pkl',\n",
       " 'CHI_2018_roster.pkl',\n",
       " 'ATL_2025_roster.pkl',\n",
       " 'NYL_2020_roster.pkl',\n",
       " 'NYL_2017_roster.pkl',\n",
       " 'IND_2021_roster.pkl',\n",
       " 'LAS_2018_roster.pkl',\n",
       " 'IND_2016_roster.pkl',\n",
       " 'MIN_2024_roster.pkl',\n",
       " 'CON_2024_roster.pkl',\n",
       " 'CHI_2022_roster.pkl',\n",
       " 'WAS_2022_roster.pkl',\n",
       " 'IND_2024_roster.pkl',\n",
       " 'DAL_2018_roster.pkl',\n",
       " 'CON_2021_roster.pkl',\n",
       " 'MIN_2016_roster.pkl',\n",
       " 'MIN_2021_roster.pkl',\n",
       " 'CON_2016_roster.pkl',\n",
       " 'NYL_2025_roster.pkl',\n",
       " 'ATL_2017_roster.pkl',\n",
       " 'SEA_2022_roster.pkl',\n",
       " 'ATL_2020_roster.pkl',\n",
       " 'PHO_2023_roster.pkl',\n",
       " 'SEA_2018_roster.pkl',\n",
       " 'DAL_2022_roster.pkl',\n",
       " 'PHO_2019_roster.pkl',\n",
       " 'LVA_2024_roster.pkl',\n",
       " 'CHI_2024_roster.pkl',\n",
       " 'PHO_2017_roster.pkl',\n",
       " 'PHO_2020_roster.pkl',\n",
       " 'SAS_2016_roster.pkl',\n",
       " 'WAS_2024_roster.pkl',\n",
       " 'ATL_2023_roster.pkl',\n",
       " 'SEA_2021_roster.pkl',\n",
       " 'SEA_2016_roster.pkl',\n",
       " 'MIN_2022_roster.pkl',\n",
       " 'CON_2022_roster.pkl',\n",
       " 'DAL_2021_roster.pkl',\n",
       " 'DAL_2016_roster.pkl',\n",
       " 'CON_2018_roster.pkl',\n",
       " 'MIN_2018_roster.pkl',\n",
       " 'LAS_2024_roster.pkl',\n",
       " 'ATL_2019_roster.pkl',\n",
       " 'LVA_2022_roster.pkl',\n",
       " 'NYL_2019_roster.pkl',\n",
       " 'DAL_2024_roster.pkl',\n",
       " 'LAS_2021_roster.pkl',\n",
       " 'IND_2018_roster.pkl',\n",
       " 'LAS_2016_roster.pkl',\n",
       " 'WAS_2016_roster.pkl',\n",
       " 'WAS_2021_roster.pkl',\n",
       " 'LVA_2018_roster.pkl',\n",
       " 'CHI_2016_roster.pkl',\n",
       " 'PHO_2025_roster.pkl',\n",
       " 'CHI_2021_roster.pkl',\n",
       " 'IND_2022_roster.pkl',\n",
       " 'SEA_2024_roster.pkl',\n",
       " 'NYL_2023_roster.pkl',\n",
       " 'NYL_2024_roster.pkl',\n",
       " 'ATL_2016_roster.pkl',\n",
       " 'ATL_2021_roster.pkl',\n",
       " 'SEA_2023_roster.pkl',\n",
       " 'IND_2025_roster.pkl',\n",
       " 'DAL_2019_roster.pkl',\n",
       " 'MIN_2017_roster.pkl',\n",
       " 'CON_2020_roster.pkl',\n",
       " 'CON_2017_roster.pkl',\n",
       " 'MIN_2020_roster.pkl',\n",
       " 'PHO_2022_roster.pkl',\n",
       " 'DAL_2023_roster.pkl',\n",
       " 'SEA_2019_roster.pkl',\n",
       " 'LVA_2025_roster.pkl',\n",
       " 'GSV_2025_roster.pkl',\n",
       " 'PHO_2018_roster.pkl',\n",
       " 'LAS_2023_roster.pkl',\n",
       " 'CHI_2019_roster.pkl',\n",
       " 'LVA_2020_roster.pkl',\n",
       " 'WAS_2019_roster.pkl',\n",
       " 'LAS_2019_roster.pkl',\n",
       " 'IND_2020_roster.pkl',\n",
       " 'IND_2017_roster.pkl',\n",
       " 'MIN_2025_roster.pkl',\n",
       " 'CON_2025_roster.pkl',\n",
       " 'ATL_2024_roster.pkl',\n",
       " 'NYL_2021_roster.pkl',\n",
       " 'NYL_2016_roster.pkl',\n",
       " 'WAS_2023_roster.pkl',\n",
       " 'CHI_2023_roster.pkl']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80049a78-28f3-4978-be1d-ffe3533afdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/rosters/WNBA/'\n",
    "files = [i for i in os.listdir(folder) if '2025' in i]\n",
    "dfs = list()\n",
    "for file in files:\n",
    "    fp = os.path.join(folder, file)\n",
    "    df = pd.read_pickle(fp)\n",
    "    dfs.append(df)\n",
    "\n",
    "final = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df = final.copy()\n",
    "table = 'dim_rosters'\n",
    "temp_config = db_config[table]\n",
    "df = df[temp_config['df_cols']]\n",
    "df.columns = temp_config['rename_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0393c4d0-1804-4048-b186-d1e736ceb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df(final, 'dim_players', conn_string, ['id'], db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2513df46-6f96-4aa1-8b66-a5e9ed596d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('db_backup/dim_rosters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ae9dc4c-d6ed-4e1b-b2e8-e997d2e92a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final.copy()\n",
    "folder = 'db_backup/'\n",
    "table = 'f_advanced_box_score'\n",
    "temp_config = db_config[table]\n",
    "df = df[temp_config['df_cols']]\n",
    "df.columns = temp_config['rename_cols']\n",
    "fp = os.path.join(folder, '{}.csv'.format(table))\n",
    "df = df.drop_duplicates(subset='id')\n",
    "df.to_csv(fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bee8949c-7ed3-4eee-b4c4-bcbb92dfeb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>player_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>jersey_number</th>\n",
       "      <th>playing_position</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>years_pro</th>\n",
       "      <th>college</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bueckpa01w-DAL-2025</td>\n",
       "      <td>bueckpa01w</td>\n",
       "      <td>DAL</td>\n",
       "      <td>5</td>\n",
       "      <td>G</td>\n",
       "      <td>72</td>\n",
       "      <td></td>\n",
       "      <td>October 20, 2001</td>\n",
       "      <td>R</td>\n",
       "      <td>UConn</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carridi01w-DAL-2025</td>\n",
       "      <td>carridi01w</td>\n",
       "      <td>DAL</td>\n",
       "      <td>21</td>\n",
       "      <td>G-F</td>\n",
       "      <td>71</td>\n",
       "      <td>175</td>\n",
       "      <td>January 8, 1998</td>\n",
       "      <td>4</td>\n",
       "      <td>Baylor</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>charlka01w-DAL-2025</td>\n",
       "      <td>charlka01w</td>\n",
       "      <td>DAL</td>\n",
       "      <td>3</td>\n",
       "      <td>G-F</td>\n",
       "      <td>73</td>\n",
       "      <td>168</td>\n",
       "      <td>March 23, 1998</td>\n",
       "      <td>4</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geiselu01w-DAL-2025</td>\n",
       "      <td>geiselu01w</td>\n",
       "      <td>DAL</td>\n",
       "      <td>18</td>\n",
       "      <td>C</td>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td>February 10, 2000</td>\n",
       "      <td>R</td>\n",
       "      <td>None</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>harrity01w-DAL-2025</td>\n",
       "      <td>harrity01w</td>\n",
       "      <td>DAL</td>\n",
       "      <td>52</td>\n",
       "      <td>G</td>\n",
       "      <td>70</td>\n",
       "      <td>152</td>\n",
       "      <td>May 1, 1998</td>\n",
       "      <td>5</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>nelsool01w-CON-2025</td>\n",
       "      <td>nelsool01w</td>\n",
       "      <td>CON</td>\n",
       "      <td>10</td>\n",
       "      <td>C</td>\n",
       "      <td>77</td>\n",
       "      <td>176</td>\n",
       "      <td>August 17, 2000</td>\n",
       "      <td>3</td>\n",
       "      <td>UConn</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>parksro01w-CON-2025</td>\n",
       "      <td>parksro01w</td>\n",
       "      <td>CON</td>\n",
       "      <td>21</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "      <td>170</td>\n",
       "      <td>July 19, 1992</td>\n",
       "      <td>1</td>\n",
       "      <td>VCU</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>peterha01w-CON-2025</td>\n",
       "      <td>peterha01w</td>\n",
       "      <td>CON</td>\n",
       "      <td>7</td>\n",
       "      <td>F-G</td>\n",
       "      <td>75</td>\n",
       "      <td>178</td>\n",
       "      <td>September 17, 1992</td>\n",
       "      <td>3</td>\n",
       "      <td>Duke</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>riversa01w-CON-2025</td>\n",
       "      <td>riversa01w</td>\n",
       "      <td>CON</td>\n",
       "      <td>22</td>\n",
       "      <td>G</td>\n",
       "      <td>73</td>\n",
       "      <td></td>\n",
       "      <td>March 4, 2003</td>\n",
       "      <td>R</td>\n",
       "      <td>NC State</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>sheldja01w-CON-2025</td>\n",
       "      <td>sheldja01w</td>\n",
       "      <td>CON</td>\n",
       "      <td>4</td>\n",
       "      <td>G</td>\n",
       "      <td>70</td>\n",
       "      <td>140</td>\n",
       "      <td>August 23, 2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Ohio State</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id   player_id team_id jersey_number playing_position  \\\n",
       "0    bueckpa01w-DAL-2025  bueckpa01w     DAL             5                G   \n",
       "1    carridi01w-DAL-2025  carridi01w     DAL            21              G-F   \n",
       "2    charlka01w-DAL-2025  charlka01w     DAL             3              G-F   \n",
       "3    geiselu01w-DAL-2025  geiselu01w     DAL            18                C   \n",
       "4    harrity01w-DAL-2025  harrity01w     DAL            52                G   \n",
       "..                   ...         ...     ...           ...              ...   \n",
       "149  nelsool01w-CON-2025  nelsool01w     CON            10                C   \n",
       "150  parksro01w-CON-2025  parksro01w     CON            21                F   \n",
       "151  peterha01w-CON-2025  peterha01w     CON             7              F-G   \n",
       "152  riversa01w-CON-2025  riversa01w     CON            22                G   \n",
       "153  sheldja01w-CON-2025  sheldja01w     CON             4                G   \n",
       "\n",
       "     height weight          birth_date years_pro         college season  \n",
       "0        72           October 20, 2001         R           UConn   2025  \n",
       "1        71    175     January 8, 1998         4          Baylor   2025  \n",
       "2        73    168      March 23, 1998         4        Maryland   2025  \n",
       "3        76          February 10, 2000         R            None   2025  \n",
       "4        70    152         May 1, 1998         5  South Carolina   2025  \n",
       "..      ...    ...                 ...       ...             ...    ...  \n",
       "149      77    176     August 17, 2000         3           UConn   2025  \n",
       "150      73    170       July 19, 1992         1             VCU   2025  \n",
       "151      75    178  September 17, 1992         3            Duke   2025  \n",
       "152      73              March 4, 2003         R        NC State   2025  \n",
       "153      70    140     August 23, 2000         1      Ohio State   2025  \n",
       "\n",
       "[154 rows x 11 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "da064ef6-d90a-482e-ab1f-dbd3ebc8dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_averages_simple(df):\n",
    "    \"\"\"\n",
    "    Simple method to calculate weighted averages\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out games with 0 minutes\n",
    "    df_played = df[df['minutes_played_int'] > 0].copy()\n",
    "    \n",
    "    # Group by player and calculate weighted averages\n",
    "    result = df_played.groupby(['player', 'team']).apply(\n",
    "        lambda group: pd.Series({\n",
    "            # Basic info\n",
    "            \n",
    "            # Weighted averages for each metric\n",
    "            'true_shooting_pct': np.average(group['true_shooting_pct'], weights=group['minutes_played_int']),\n",
    "            'effective_field_goal_pct': np.average(group['effective_field_goal_pct'], weights=group['minutes_played_int']),\n",
    "            'three_point_attempt_rate': np.average(group['three_point_attempt_rate'], weights=group['minutes_played_int'])\n",
    "            # 'defensive_rebound_pct': np.average(group['defensive_rebound_pct'], weights=group['minutes_played']),\n",
    "            # 'offensive_rebound_pct': np.average(group['offensive_rebound_pct'], weights=group['minutes_played']),\n",
    "            # 'assist_pct': np.average(group['assist_pct'], weights=group['minutes_played']),\n",
    "            # 'steal_pct': np.average(group['steal_pct'], weights=group['minutes_played']),\n",
    "            # 'usage_rate': np.average(group['usage_rate'], weights=group['minutes_played']),\n",
    "            # 'offensive_rating': np.average(group['offensive_rating'], weights=group['minutes_played']),\n",
    "            # 'defensive_rating': np.average(group['defensive_rating'], weights=group['minutes_played'])\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fd097-389a-4022-866f-7fcf0b3eb324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
